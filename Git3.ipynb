{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "from scipy import misc\n",
    "import glob\n",
    "from shutil import copyfile\n",
    "\n",
    "model_dir = os.getcwd() + '/Models/'\n",
    "content_dir = './content/' \n",
    "style_dir =  './50styles/'\n",
    "\n",
    "\n",
    "#vgg definition that conveniently let's you grab the outputs from any layer\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, pool='max'):\n",
    "        super(VGG, self).__init__()\n",
    "        #vgg modules\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        if pool == 'max':\n",
    "            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        elif pool == 'avg':\n",
    "            self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool5 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            \n",
    "    def forward(self, x, out_keys):\n",
    "        out = {}\n",
    "        out['r11'] = F.relu(self.conv1_1(x))\n",
    "        out['r12'] = F.relu(self.conv1_2(out['r11']))\n",
    "        out['p1'] = self.pool1(out['r12'])\n",
    "        out['r21'] = F.relu(self.conv2_1(out['p1']))\n",
    "        out['r22'] = F.relu(self.conv2_2(out['r21']))\n",
    "        out['p2'] = self.pool2(out['r22'])\n",
    "        out['r31'] = F.relu(self.conv3_1(out['p2']))\n",
    "        out['r32'] = F.relu(self.conv3_2(out['r31']))\n",
    "        out['r33'] = F.relu(self.conv3_3(out['r32']))\n",
    "        out['r34'] = F.relu(self.conv3_4(out['r33']))\n",
    "        out['p3'] = self.pool3(out['r34'])\n",
    "        out['r41'] = F.relu(self.conv4_1(out['p3']))\n",
    "        out['r42'] = F.relu(self.conv4_2(out['r41']))\n",
    "        out['r43'] = F.relu(self.conv4_3(out['r42']))\n",
    "        out['r44'] = F.relu(self.conv4_4(out['r43']))\n",
    "        out['p4'] = self.pool4(out['r44'])\n",
    "        out['r51'] = F.relu(self.conv5_1(out['p4']))\n",
    "        out['r52'] = F.relu(self.conv5_2(out['r51']))\n",
    "        out['r53'] = F.relu(self.conv5_3(out['r52']))\n",
    "        out['r54'] = F.relu(self.conv5_4(out['r53']))\n",
    "        out['p5'] = self.pool5(out['r54'])\n",
    "        return [out[key] for key in out_keys]\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class Cov_Mean(nn.Module):\n",
    "    def forward(self, input):\n",
    "        b,c,h,w = input.size()\n",
    "        F = input.view(b, c, h*w)\n",
    "        mean_ = F.mean( dim=2, keepdim=True).detach()\n",
    "        mean = torch.cat(h*w*[mean_], 2)\n",
    "        F = F-mean.detach()\n",
    "        G = torch.bmm(F, F.transpose(1,2)) \n",
    "        G.div_(h*w)\n",
    "        return G.squeeze(0).data, mean_.squeeze().data \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# pre and post processing for images\n",
    "img_size = 512 \n",
    "prep = transforms.Compose([transforms.Scale(img_size),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n",
    "                           transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n",
    "                                                std=[1,1,1]),\n",
    "                           transforms.Lambda(lambda x: x.mul_(255)),\n",
    "                          ])\n",
    "postpa = transforms.Compose([transforms.Lambda(lambda x: x.mul_(1./255)),\n",
    "                           transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], #add imagenet mean\n",
    "                                                std=[1,1,1]),\n",
    "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to RGB\n",
    "                           ])\n",
    "postpb = transforms.Compose([transforms.ToPILImage()])\n",
    "def postp(tensor): # to clip results in the range [0,1]\n",
    "    t = postpa(tensor)\n",
    "    t[t>1] = 1    \n",
    "    t[t<0] = 0\n",
    "    img = postpb(t)\n",
    "    return img\n",
    "\n",
    "#get network\n",
    "vgg = VGG()\n",
    "vgg.load_state_dict(torch.load(model_dir + 'vgg_conv.pth'))\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "if torch.cuda.is_available():\n",
    "    vgg.cuda()\n",
    "\n",
    "references=[]\n",
    "Reference_dir = './content/'\n",
    "references = glob.glob(Reference_dir+\"*.jpg\")                      \n",
    "\n",
    "Total_Covs=[0,0,0,0,0]\n",
    "style_layers = ['r11','r21','r31','r41', 'r51'] \n",
    "\n",
    "for sample in references:    \n",
    "\n",
    "    img_torch = prep(Image.open(sample)) \n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        img_torch = Variable(img_torch.unsqueeze(0).cuda()) \n",
    "    else:\n",
    "        img_torch = Variable(img_torch.unsqueeze(0))\n",
    "        sample\n",
    "    Covs_Means = [Cov_Mean()(A) for A in vgg(img_torch, style_layers[:])]\n",
    "    Total_Covs= [x+y[0] for x,y in zip(Total_Covs,Covs_Means)] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVG_Covs = [x/len(references) for x in Total_Covs]\n",
    "\n",
    "def PCA(A):\n",
    "        U,S,V = torch.svd(A)\n",
    "        return U,S,V \n",
    "    \n",
    "PCA_basis = [PCA(data) for data in AVG_Covs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 344010 style6_content344010_weight11.0_weight100.0_iteration799.png\n",
      "6 344010 style6_content344010_weight11.0_weight150.0_iteration799.png\n",
      "10 141012 style10_content141012_weight11.0_weight0.1_iteration699.png\n",
      "10 141012 style10_content141012_weight11.0_weight0.5_iteration799.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Ks = [ 32,48,128,256,256]\n",
    "\n",
    "file1 = open('sample.txt', 'r') \n",
    "file2= open('EValue%s.txt'%Ks, 'w')\n",
    "columns = [\"style\", \"content\",\"weight\",\"E1\",\"E2\",\"E3\",\"E4\",\"E5\",\"\\n\"]\n",
    "name = '\\t'.join(columns) \n",
    "file2.write(name)\n",
    "\n",
    "\n",
    "for line in file1.readlines()[:]:\n",
    "    \n",
    "    filename  = line[:-1] \n",
    "    sp =line[:].split('_')\n",
    "    style =int(sp[0][5:]) \n",
    "    content = int(sp[1][7:])\n",
    "    print(style,content, filename)\n",
    " \n",
    "    source_dir = './sample/'\n",
    "    img_dirs = [style_dir, content_dir, source_dir]\n",
    "    img_names = ['styles - %s.jpg'%style, '%s.jpg'%content, filename]\n",
    "    imgs = [Image.open(img_dirs[i] + name) for i,name in enumerate(img_names)]\n",
    "    imgs_torch = [prep(img) for img in imgs]\n",
    "    if torch.cuda.is_available():\n",
    "        imgs_torch = [Variable(img.unsqueeze(0).cuda()) for img in imgs_torch]\n",
    "    else:\n",
    "        imgs_torch = [Variable(img.unsqueeze(0)) for img in imgs_torch]\n",
    "    style_image, content_image, syn_image= imgs_torch\n",
    "\n",
    "\n",
    "    style_layers = ['r11','r21','r31','r41', 'r51'] \n",
    "    content_layers = ['r42']  \n",
    "\n",
    "    style_targets = [Cov_Mean()(A) for A in vgg(style_image, style_layers[:])]\n",
    "    syn_results = [Cov_Mean()(A) for A in vgg(syn_image, style_layers[:])]\n",
    "\n",
    "\n",
    "    def PCA_Proj(A,P,k):\n",
    "        return torch.mm(torch.mm(P[0][:,:k].t(),A[0]),P[2][:,:k]), torch.mm( A[1].unsqueeze(0), P[0][:,:k] ) \n",
    "    \n",
    "    PCA_targets = [PCA_Proj(data,P ,k) for data,P,k in zip(style_targets,PCA_basis,Ks)]\n",
    "    PCA_syn_results = [PCA_Proj(data,P ,k) for data,P,k in zip(syn_results,PCA_basis,Ks)]     \n",
    "  \n",
    "    def Det(A,B):\n",
    "        _,S,_ = torch.svd(A)\n",
    "        _,S1,_ = torch.svd(B)\n",
    "        temp =torch.log(S1/S)\n",
    "        u=0\n",
    "        for a in temp:\n",
    "            u +=a\n",
    "        return u\n",
    "    \n",
    "    LogDet_AoverB = [ Det(syn[0],tar[0]) for syn,tar,k in zip(PCA_syn_results,PCA_targets,Ks)]\n",
    "\n",
    "    KLs = []\n",
    "\n",
    "    KL_parts  = [ (torch.trace(torch.mm( y[0].inverse(), x[0])), torch.mm( torch.mm((y[1] -x[1]),  y[0].inverse()), (y[1]-x[1]).t() ).squeeze()[0] ,-k, logD) for x,y,logD, k in zip(PCA_syn_results,PCA_targets,LogDet_AoverB,Ks)]\n",
    "\n",
    "    KLs.append(np.sum(x) for x in KL_parts )  # np.sum(x) gives the 2 KL divergence\n",
    "    KL_array = np.array(KLs)\n",
    "    Es = [ str(-np.log(x)+ np.log(2)) for x in KLs[0]] # E value is -log(KL)\n",
    "    new = sp[:3]+Es +[\"\\n\"]  \n",
    " \n",
    "    name = '\\t'.join(new) \n",
    "    file2.write(name)\n",
    "file1.close()\n",
    "file2.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 344010\n",
      "6 344010\n",
      "10 141012\n",
      "10 141012\n"
     ]
    }
   ],
   "source": [
    "file1 = open('sample.txt', 'r') \n",
    "\n",
    "for line in file1.readlines()[:]:\n",
    "    sp =line[:].split('_')\n",
    "    style =int(sp[0][5:]) \n",
    "\n",
    "    content = int(sp[1][7:])\n",
    "    print(style,content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
